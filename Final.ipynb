{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 1: Perform the following operations using Python on a data set : read data\n",
    "# from different formats(like csv, xls),indexing and selecting data, sort data,\n",
    "# describe attributes of data, checking data types of each column. (Use\n",
    "# Titanic Dataset).\n",
    "import pandas as pd\n",
    "# Read Data\n",
    "csvdf=pd.read_csv(\"titanic.csv\")\n",
    "exceldf=pd.read_excel(\"titanic.xlsx\")\n",
    "# Indexing and Selecting\n",
    "csvdf.iloc[0:4]\n",
    "csvdf.iloc[-2:]\n",
    "csvdf['Name']\n",
    "csvdf[['Name',\"Pclass\"]]\n",
    "exceldf[[\"Name\",\"Pclass\"]]\n",
    "# sort data\n",
    "csvdf.sort_values(by=\"Age\",ascending=False)\n",
    "exceldf.sort_values(by=\"Name\")\n",
    "# describe attributes of data\n",
    "csvdf.describe()\n",
    "exceldf.describe()\n",
    "# data types\n",
    "csvdf.dtypes\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#2: Perform the following operations using Python on the Telecom_Churn\n",
    "# dataset. Compute and display summary statistics for each feature available\n",
    "# in the dataset using separate commands for each statistic. (e.g. minimum\n",
    "# value, maximum value, mean, range, standard deviation, variance and\n",
    "# percentiles).\n",
    "\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"telecom_churn.csv\")\n",
    "# minimum\n",
    "df.min(numeric_only=True)\n",
    "# maximum\n",
    "df.max(numeric_only=True)\n",
    "# mean\n",
    "df.mean(numeric_only=True)\n",
    "# range\n",
    "df.max(numeric_only=True)-df.min(numeric_only=True)\n",
    "# standard deviation\n",
    "df.std(numeric_only=True)\n",
    "# variance\n",
    "df.var(numeric_only=True)\n",
    "# percentiles\n",
    "df.quantile([0.25,0.50,0.75],numeric_only=True)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3:Perform the following operations using Python on the data set\n",
    "# House_Price Prediction dataset. Compute standard deviation, variance and\n",
    "# percentiles using separate commands, for each feature. Create a histogram\n",
    "# for each feature in the dataset to illustrate the feature distributions.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df=pd.read_csv(\"house_price.csv\")\n",
    "# standard deviation\n",
    "df.std(numeric_only=True)\n",
    "# variance\n",
    "df.var(numeric_only=True)\n",
    "# percentiles\n",
    "df.quantile([.25,.50,.75],numeric_only=True)\n",
    "# histogram for each feature\n",
    "df.hist(figsize=(10,12))\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4. Write a program to do: A dataset collected in a cosmetics shop showing\n",
    "# details of customers and whether or not they responded to a special offer\n",
    "# to buy a new lip-stick is shown in table below. (Implement step by step\n",
    "# using commands - Dont use library) Use this dataset to build a decision\n",
    "# tree, with Buys as the target variable, to help in buying lipsticks in the\n",
    "# future. Find the root node of the decision tree.\n",
    "\n",
    "import csv\n",
    "import math\n",
    "\n",
    "# Read CSV file\n",
    "data = []\n",
    "with open(\"cosmetics.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "\n",
    "# target column\n",
    "target = header.index(\"Buys\")\n",
    "\n",
    "# Calculate entropy\n",
    "def entropy(rows):\n",
    "    total = len(rows)\n",
    "    yes = sum(1 for r in rows if r[target] == \"Yes\")\n",
    "    no = total - yes\n",
    "    if yes == 0 or no == 0:\n",
    "        return 0\n",
    "    p_yes = yes / total\n",
    "    p_no = no / total\n",
    "    return -(p_yes*math.log2(p_yes) + p_no*math.log2(p_no))\n",
    "\n",
    "# Information gain\n",
    "def info_gain(rows, col):\n",
    "    base_entropy = entropy(rows)\n",
    "    values = {}\n",
    "    for r in rows:\n",
    "        values.setdefault(r[col], []).append(r)\n",
    "\n",
    "    remainder = 0\n",
    "    for v in values.values():\n",
    "        remainder += (len(v)/len(rows)) * entropy(v)\n",
    "\n",
    "    return base_entropy - remainder\n",
    "\n",
    "# Compute IG for all attributes except Buys\n",
    "best_attr = None\n",
    "best_ig = -1\n",
    "\n",
    "for i, col in enumerate(header):\n",
    "    if col == \"Buys\": \n",
    "        continue\n",
    "    ig = info_gain(data, i)\n",
    "    print(col, \"-> IG =\", round(ig, 4))\n",
    "    if ig > best_ig:\n",
    "        best_ig = ig\n",
    "        best_attr = col\n",
    "\n",
    "print(\"\\nRoot Node =\", best_attr)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5,6,7,8: Write a program to do: A dataset collected in a cosmetics shop showing\n",
    "# details of customers and whether or not they responded to a special offer\n",
    "# to buy a new lip-stick is shown in table below. (Use library commands)\n",
    "# According to the decision tree you have made from the previous training\n",
    "# data set, what is the decision for the test data: [Age &lt; 21, Income = Low,\n",
    "# Gender = Female, Marital Status = Married]?\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "df=pd.read_csv(\"cosmetics.csv\")\n",
    "# Encode the Data\n",
    "encoder={}\n",
    "for col in [\"Income\",\"Gender\",\"MaritalStatus\",\"Buys\"]:\n",
    "    le=LabelEncoder()\n",
    "    df[col]=le.fit_transform(df[col])\n",
    "    encoder[col]=le\n",
    "\n",
    "# Tree\n",
    "model=DecisionTreeClassifier()\n",
    "x=df.drop(\"Buys\",axis=1)\n",
    "y=df[\"Buys\"]\n",
    "model.fit(x,y)\n",
    "\n",
    "# Test Data\n",
    "test=pd.DataFrame({\n",
    "    \"Age\":[21],\n",
    "    \"Income\":[\"Low\"],\n",
    "    \"Gender\":[\"Female\"],\n",
    "    \"MaritalStatus\":[\"Married\"]\n",
    "})\n",
    "\n",
    "# Encode test data\n",
    "for col in [\"Income\",\"Gender\",\"MaritalStatus\"]:\n",
    "    test[col]=encoder[col].transform(test[col])\n",
    "\n",
    "result=model.predict(test)[0]\n",
    "if result==1:\n",
    "    print(\"YES\")\n",
    "else:\n",
    "    print(\"NO\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 9. Write a program to do the following: You have given a collection of 8\n",
    "# points. P1=[0.1,0.6] P2=[0.15,0.71] P3=[0.08,0.9] P4=[0.16, 0.85]\n",
    "# P5=[0.2,0.3] P6=[0.25,0.5] P7=[0.24,0.1] P8=[0.3,0.2]. Perform the k-mean\n",
    "# clustering with initial centroids as m1=P1 =Cluster#1=C1 and\n",
    "# m2=P8=cluster#2=C2. Answer the following 1] Which cluster does P6\n",
    "# belong to? 2] What is the population of a cluster around m2? 3] What is\n",
    "# the updated value of m1 and m2?\n",
    "\n",
    "import math\n",
    "points={\n",
    "    \"P1\":[0.1,0.6],\n",
    "    \"P2\":[0.15,0.71],\n",
    "    \"P3\":[0.08,0.9],\n",
    "    \"P4\":[0.16, 0.85],\n",
    "    \"P5\":[0.2,0.3],\n",
    "    \"P6\":[0.25,0.5],\n",
    "    \"P7\":[0.24,0.1],\n",
    "    \"P8\":[0.3,0.2]\n",
    "}\n",
    "m1=points[\"P1\"]\n",
    "m2=points[\"P8\"]\n",
    "\n",
    "c1=[]\n",
    "c2=[]\n",
    "\n",
    "def dist(a,b):\n",
    "    return math.sqrt( (a[0]-b[0])**2 + (a[1]-b[1])**2)\n",
    "\n",
    "for name,point in points.items():\n",
    "    d1=dist(m1,point)\n",
    "    d2=dist(m2,point)\n",
    "    if d1<d2:\n",
    "        c1.append(name)\n",
    "    else:\n",
    "        c2.append(name)\n",
    "\n",
    "print(\"C1\", c1)\n",
    "print(\"C2\", c2)\n",
    "\n",
    "for c in c1:\n",
    "    if c==\"P6\":\n",
    "        print(\"P6 in C1\")\n",
    "\n",
    "for c in c2:\n",
    "    if c==\"P6\":\n",
    "        print(\"P6 in C2\")\n",
    "\n",
    "print(\"Population of a cluster around m2 is:\",len(c2))   \n",
    "\n",
    "x1=sum(points[c][0] for c in c1)/len(c1)\n",
    "x2=sum(points[c][1] for c in c1)/len(c1)\n",
    "\n",
    "y1=sum(points[c][0] for c in c2)/len(c2)\n",
    "y2=sum(points[c][1] for c in c2)/len(c2)\n",
    "\n",
    "print(\"Updated value of m1 is:\",[x1,x2])\n",
    "print(\"Updated value of m2 is:\",[y1,y2])    \n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 10. Write a program to do the following: You have given a collection of 8\n",
    "# points. P1=[2, 10] P2=[2, 5] P3=[8, 4] P4=[5, 8] P5=[7,5] P6=[6, 4] P7=[1, 2]\n",
    "# P8=[4, 9]. Perform the k-mean clustering with initial centroids as m1=P1\n",
    "# =Cluster#1=C1 and m2=P4=cluster#2=C2, m3=P7 =Cluster#3=C3. Answer\n",
    "# the following 1] Which cluster does P6 belong to? 2] What is the\n",
    "# population of a cluster around m3? 3] What is the updated value of m1,\n",
    "# m2, m3?\n",
    "\n",
    "import math\n",
    "points={\n",
    "    \"P1\":[2, 10],\n",
    "    \"P2\":[2, 5],\n",
    "    \"P3\":[8,4],\n",
    "    \"P4\":[5,8],\n",
    "    \"P5\":[7,5],\n",
    "    \"P6\":[6,4],\n",
    "    \"P7\":[1,2],\n",
    "    \"P8\":[4,9],\n",
    "}\n",
    "m1=points[\"P1\"]\n",
    "m2=points[\"P4\"]\n",
    "m3=points[\"P7\"]\n",
    "\n",
    "c1=[]\n",
    "c2=[]\n",
    "c3=[]\n",
    "\n",
    "def dist(a,b):\n",
    "    return math.sqrt((a[0]-b[0])**2 + (a[1]-b[1])**2)\n",
    "\n",
    "for name, point in points.items():\n",
    "    d1=dist(point,m1)\n",
    "    d2=dist(point,m2)\n",
    "    d3=dist(point,m3)\n",
    "    d_min=min(d1,d2,d3)\n",
    "\n",
    "    if d_min==d1:\n",
    "        c1.append(name)\n",
    "    elif d_min==d2:\n",
    "        c2.append(name)\n",
    "    else:\n",
    "        c3.append(name)\n",
    "\n",
    "\n",
    "print(\"C1:\",c1)\n",
    "print(\"C2:\",c2)\n",
    "print(\"C3:\",c3)\n",
    "\n",
    "#  Which cluster does P6 belong to\n",
    "for c in c1:\n",
    "    if c==\"P6\":\n",
    "        print(\"P6 present in C1\")\n",
    "\n",
    "for c in c2:\n",
    "    if c==\"P6\":\n",
    "        print(\"P6 present in C2\")   \n",
    "\n",
    "for c in c3:\n",
    "    if c==\"P6\":\n",
    "        print(\"P6 present in C3\")\n",
    "\n",
    "\n",
    "print(\"Population of C3:\",len(c3))\n",
    "\n",
    "x1=sum(points[c][0] for c in c1)/len(c1)\n",
    "x2=sum(points[c][1] for c in c1)/len(c1)\n",
    "\n",
    "y1=sum(points[c][0] for c in c2)/len(c2)\n",
    "y2=sum(points[c][1] for c in c2)/len(c2)\n",
    "\n",
    "z1=sum(points[c][0] for c in c3)/len(c3)\n",
    "z2=sum(points[c][1] for c in c3)/len(c3)\n",
    "\n",
    "print(\"Update M1 :\",[x1,x2])\n",
    "print(\"Update M2 :\",[y1,y2])\n",
    "print(\"Update M3 :\",[z1,z2])\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 11. Use Iris flower dataset and perform following :\n",
    "# 1. List down the features and their types (e.g., numeric, nominal)\n",
    "# available in the dataset. 2. Create a histogram for each feature in the\n",
    "# dataset to illustrate the feature distributions.\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df=sns.load_dataset(\"iris\")\n",
    "\n",
    "print(df.dtypes)\n",
    "df.hist(figsize=(10,12))\n",
    "plt.suptitle(\"Iris Feature Distributions\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 12.Use Iris flower dataset and perform following :\n",
    "# 1. Create a box plot for each feature in the dataset.\n",
    "# 2. Identify and discuss distributions and identify outliers from them.\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df=sns.load_dataset(\"iris\")\n",
    "\n",
    "df.boxplot(figsize=(8,10))\n",
    "plt.suptitle(\"Box Plot for Iris Dataset Features\")\n",
    "plt.show()\n",
    "df.describe()\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 13. Use the covid_vaccine_statewise.csv dataset and perform the\n",
    "# following analytics.\n",
    "# a. Describe the dataset\n",
    "# b. Number of persons state wise vaccinated for first dose in India\n",
    "# c. Number of persons state wise vaccinated for second dose in India\n",
    "\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"covid_vaccine_statewise.csv\")\n",
    "# df.describe()\n",
    "# df[[\"State\",\"First Dose Administered\"]]\n",
    "# df[[\"State\",\"Second Dose Administered\"]]\n",
    "\n",
    "df[\"Updated On\"]=pd.to_datetime(df[\"Updated On\"])\n",
    "df=df.sort_values([\"State\",\"Updated On\"])\n",
    "latest=df.groupby(\"State\").tail(1)\n",
    "\n",
    "print(df.describe(include=\"all\"))\n",
    "first_dose = latest[[\"State\", \"First Dose Administered\"]]\n",
    "print(first_dose)\n",
    "second_dose = latest[[\"State\", \"Second Dose Administered\"]]\n",
    "print(second_dose)\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 14. Use the covid_vaccine_statewise.csv dataset and perform the\n",
    "# following analytics.\n",
    "# A. Describe the dataset.\n",
    "# B. Number of Males vaccinated\n",
    "# C.. Number of females vaccinated\n",
    "\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"covid_vaccine_statewise.csv\")\n",
    "print(df.describe())\n",
    "\n",
    "print(df[[\"State\",\"Male Vaccinated\"]])\n",
    "total=0\n",
    "for n in df['Male Vaccinated']:\n",
    "    total+=n\n",
    "print(\"Total Male Vaccinated:\",total)\n",
    "\n",
    "\n",
    "print(df[[\"State\",\"Female Vaccinated\"]])\n",
    "total=0\n",
    "for n in df['Female Vaccinated']:\n",
    "    total+=n\n",
    "print(\"Total Male Vaccinated:\",total)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 15. Use the dataset 'titanic'. The dataset contains 891 rows and contains\n",
    "# information about the passengers who boarded the unfortunate Titanic\n",
    "# ship. Use the Seaborn library to see if we can find any patterns in the data.\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "titanic=sns.load_dataset(\"titanic\")\n",
    "titanic.isnull().sum()\n",
    "sns.countplot(x=titanic[\"survived\"])\n",
    "plt.show()\n",
    "sns.countplot(x=titanic[\"sex\"],hue=titanic[\"survived\"])\n",
    "plt.show()\n",
    "sns.countplot(x=titanic[\"pclass\"],hue=titanic[\"survived\"])\n",
    "plt.show()\n",
    "sns.histplot(x=titanic[\"age\"],hue=titanic[\"survived\"],kde=True)\n",
    "plt.show()\n",
    "sns.boxplot(x=\"survived\", y=\"fare\", data=titanic)\n",
    "plt.show()\n",
    "# Women survived more than men\n",
    "# 1st class passengers survived more than 2nd and 3rd class\n",
    "# Children (age < 15) had higher survival rate\n",
    "# High-fare passengers survived more\n",
    "# Passengers with small families survived more\n",
    "# People who embarked from Cherbourg survived more\n",
    "# Most deaths were in adult males from 3rd class\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 16. Use the inbuilt dataset &#39;titanic&#39;. The dataset contains 891 rows and\n",
    "# contains information about the passengers who boarded the unfortunate\n",
    "# Titanic ship. Write a code to check how the price of the ticket (column\n",
    "# name:fare) for each passenger is distributed by plotting a histogram.\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "titanic=sns.load_dataset(\"titanic\")\n",
    "\n",
    "sns.histplot(x=titanic[\"fare\"],bins=30,kde=True)\n",
    "plt.title(\"Distribution of Ticket Fare\")\n",
    "plt.xlabel(\"Fare\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 17. Compute Accuracy, Error rate, Precision, Recall for following confusion\n",
    "# matrix ( Use formula for each)\n",
    "\n",
    "# Confusion Matrix Metrics (Compact One-Page Answer)\n",
    "# Given: TP=1, FP=1, FN=8, TN=90, Total=100\n",
    "# Accuracy = (TP+TN)/Total = (1+90)/100 = 0.91 = 91%\n",
    "# Meaning: Model correct 91% times.\n",
    "# Error Rate = (FP+FN)/Total = (1+8)/100 = 0.09 = 9%\n",
    "# Meaning: Model wrong 9% times.\n",
    "# Precision = TP/(TP+FP) = 1/(1+1) = 0.5 = 50%\n",
    "# Meaning: Positive predictions are 50% correct.\n",
    "# Recall = TP/(TP+FN) = 1/(1+8) ≈ 0.11 = 11%\n",
    "# Meaning: Model finds only 11% actual positives (misses many).\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 18. Use House_Price prediction dataset. Provide summary statistics (mean,\n",
    "# median, minimum, maximum, standard deviation) of variables (categorical\n",
    "# vs quantitative) such as- For example, if categorical variable is age groups\n",
    "# and quantitative variable is income, then provide summary statistics of\n",
    "# income grouped by the age groups.\n",
    "import pandas as pd\n",
    "data = {\n",
    "    \"Location\": [\"City\", \"City\", \"Village\", \"Town\", \"Town\", \"Village\", \"City\", \"Town\", \"Village\"],\n",
    "    \"House_Price\": [7500000, 8200000, 3500000, 5500000, 6000000, 4000000, 9000000, 5800000, 3800000]\n",
    "}\n",
    "df=pd.DataFrame(data)\n",
    "\n",
    "summury=df.groupby(\"Location\")[\"House_Price\"].agg([\"mean\",\"median\",\"min\",\"max\",\"std\"])\n",
    "print(summury)\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# 19. Write a Python program to display some basic statistical details like\n",
    "# percentile, mean, standard deviation etc (Use python and pandas\n",
    "# commands) the species of ‘Iris-setosa’, ‘Iris-versicolor’ and ‘Iris-versicolor’\n",
    "# of iris.csv dataset.\n",
    "\n",
    "import seaborn as sns\n",
    "df=sns.load_dataset('iris')\n",
    "species_list=[\"versicolor\",\"setosa\",\"virginica\"]\n",
    "\n",
    "for specie in species_list:\n",
    "    data=df[df[\"species\"]==specie]\n",
    "    print(data.describe())\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# 22. Compute Accuracy, Error rate, Precision, Recall for the following\n",
    "# confusion matrix.\n",
    "# TP = 90\n",
    "# FN = 210\n",
    "# FP = 140\n",
    "# TN = 9560\n",
    "# Total = 10000\n",
    "# Formulas & Answers\n",
    "# Accuracy = (TP + TN) / Total = (90 + 9560) / 10000 = 96.5%\n",
    "# Error Rate = (FP + FN) / Total = (140 + 210) / 10000 = 3.5%\n",
    "# Precision = TP / (TP + FP) = 90 / 230 = 39.1%\n",
    "# Recall = TP / (TP + FN) = 90 / 300 = 30%\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 23. With reference to Table , obtain the Frequency table for the\n",
    "# attribute age. From the frequency table you have obtained, calculate\n",
    "# the information gain of the frequency table while splitting on Age. (Use\n",
    "# step by step Python/Pandas commands)\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# STEP 1: Create DataFrame\n",
    "data = {\n",
    "    \"Age\": [\"Young\",\"Young\",\"Middle\",\"Old\",\"Old\",\"Old\",\"Middle\",\"Young\",\n",
    "            \"Young\",\"Old\",\"Young\",\"Middle\",\"Middle\",\"Old\"],\n",
    "    \"Income\": [\"High\",\"High\",\"High\",\"Medium\",\"Low\",\"Low\",\"Low\",\"Medium\",\n",
    "               \"Low\",\"Medium\",\"Medium\",\"Medium\",\"High\",\"Medium\"],\n",
    "    \"Married\": [\"No\",\"No\",\"No\",\"No\",\"Yes\",\"Yes\",\"No\",\"No\",\n",
    "                \"Yes\",\"Yes\",\"No\",\"No\",\"Yes\",\"No\"],\n",
    "    \"Health\": [\"Fair\",\"Good\",\"Fair\",\"Fair\",\"Fair\",\"Good\",\"Good\",\"Fair\",\n",
    "               \"Fair\",\"Fair\",\"Fair\",\"Good\",\"Fair\",\"Good\"],\n",
    "    \"Class\": [\"No\",\"No\",\"Yes\",\"Yes\",\"Yes\",\"No\",\"Yes\",\"No\",\n",
    "              \"Yes\",\"Yes\",\"Yes\",\"Yes\",\"Yes\",\"No\"]\n",
    "}\n",
    "df=pd.DataFrame(data)\n",
    "\n",
    "# STEP 2: Frequency Table for Age\n",
    "freq=df[\"Age\"].value_counts()\n",
    "print(freq)\n",
    "\n",
    "# STEP 3: Entropy Function\n",
    "def entropy(column):\n",
    "    values=column.value_counts(normalize=True)\n",
    "    return -sum(p * math.log2(p) for p in values)\n",
    "\n",
    "# STEP 4: Total Entropy of dataset\n",
    "overall_entropy=entropy(df[\"Class\"])\n",
    "print(overall_entropy)\n",
    "\n",
    "# STEP 5: Entropy of each Age group\n",
    "entropy_age_groups=df.groupby(\"Age\")[\"Class\"].apply(entropy)\n",
    "print(entropy_age_groups)\n",
    "\n",
    "# STEP 6: Weighted Entropy after split on Age\n",
    "total_rows=len(df)\n",
    "weighted_entropy = sum(\n",
    "    (df[df[\"Age\"]==age].shape[0]/total_rows)*entropy_age_groups[age]\n",
    "    for age in entropy_age_groups.index\n",
    ")\n",
    "print(weighted_entropy)\n",
    "\n",
    "# STEP 7: Information Gain\n",
    "information_gain=overall_entropy-weighted_entropy\n",
    "print(\"\\nInformation Gain (Age):\",information_gain)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 24. Perform the following operations using Python on a suitable data set,\n",
    "# counting unique values of data, format of each column, converting\n",
    "# variable data type (e.g. from long to short, vice versa), identifying missing\n",
    "# values and filling in the missing values.\n",
    "\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"students.csv\")\n",
    "\n",
    "# Count unique values of each column\n",
    "print(\"Unique values:\")\n",
    "print(df.nunique())\n",
    "\n",
    "# format of each column\n",
    "print(\"\\nColumn Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# converting variable data type (e.g. from long to short, vice versa)\n",
    "df[\"Age\"]=df[\"Age\"].astype(\"float\")\n",
    "df[\"Passed\"]=df[\"Passed\"].astype(\"category\")\n",
    "print(\"\\nData Types After conversion\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# identifying missing values \n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Fill the Missing the values\n",
    "df[\"Marks\"].fillna(df[\"Marks\"].min(),inplace=True)\n",
    "df[\"Height\"].fillna(df[\"Height\"].median(),inplace=True)\n",
    "print(\"\\nAfter Filling Missing Values:\")\n",
    "df\n",
    "# -----------------------------------------------------------------------------\n",
    "# 25. Perform Data Cleaning, Data transformation using Python on any data\n",
    "# set.\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"data_cleaning.csv\")\n",
    "print(df)\n",
    "# 2. Check format (data types)\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "# 3. Count unique values\n",
    "print(\"\\nUnique values in each column:\")\n",
    "print(df.nunique())\n",
    "# 4. Identify missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "# 5. Fill missing Age with mean\n",
    "df[\"Age\"]=df[\"Age\"].fillna(df[\"Age\"].mean())\n",
    "df[\"Salary\"]=df[\"Salary\"].fillna(df[\"Salary\"].median())\n",
    "df[\"City\"]=df[\"City\"].fillna(\"Unknown\")\n",
    "# 6. Convert Age from float to int\n",
    "df[\"Age\"]=df[\"Age\"].astype(int)\n",
    "df[\"Salary\"]=df[\"Salary\"].astype(int)\n",
    "# 7. Transformation\n",
    "df[\"Name\"]=df[\"Name\"].str.upper()\n",
    "# 8. Print Clean data\n",
    "print(\"\\nCleaned & Transformed Dataset:\")\n",
    "df\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 4. Write a program to do: A dataset collected in a cosmetics shop showing\n",
    "# details of customers and whether or not they responded to a special offer\n",
    "# to buy a new lip-stick is shown in table below. (Implement step by step\n",
    "# using commands - Dont use library) Use this dataset to build a decision\n",
    "# tree, with Buys as the target variable, to help in buying lipsticks in the\n",
    "# future. Find the root node of the decision tree.\n",
    "\n",
    "import csv\n",
    "import math\n",
    "\n",
    "# Read CSV file\n",
    "data = []\n",
    "with open(\"cosmetics.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "\n",
    "# target column\n",
    "target = header.index(\"Buys\")\n",
    "\n",
    "# Calculate entropy\n",
    "def entropy(rows):\n",
    "    total = len(rows)\n",
    "    yes = sum(1 for r in rows if r[target] == \"Yes\")\n",
    "    no = total - yes\n",
    "    if yes == 0 or no == 0:\n",
    "        return 0\n",
    "    p_yes = yes / total\n",
    "    p_no = no / total\n",
    "    return -(p_yes*math.log2(p_yes) + p_no*math.log2(p_no))\n",
    "\n",
    "# Information gain\n",
    "def info_gain(rows, col):\n",
    "    base_entropy = entropy(rows)\n",
    "    values = {}\n",
    "    for r in rows:\n",
    "        values.setdefault(r[col], []).append(r)\n",
    "\n",
    "    remainder = 0\n",
    "    for v in values.values():\n",
    "        remainder += (len(v)/len(rows)) * entropy(v)\n",
    "\n",
    "    return base_entropy - remainder\n",
    "\n",
    "# Compute IG for all attributes except Buys\n",
    "best_attr = None\n",
    "best_ig = -1\n",
    "\n",
    "for i, col in enumerate(header):\n",
    "    if col == \"Buys\": \n",
    "        continue\n",
    "    ig = info_gain(data, i)\n",
    "    print(col, \"-> IG =\", round(ig, 4))\n",
    "    if ig > best_ig:\n",
    "        best_ig = ig\n",
    "        best_attr = col\n",
    "\n",
    "print(\"\\nRoot Node =\", best_attr)\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 20:Write a program to cluster a set of points using K-means for IRIS\n",
    "# dataset. Consider, K=3, clusters. Consider Euclidean distance as the\n",
    "# distance measure. Randomly initialize a cluster mean as one of the data\n",
    "# points. Iterate at least for 10 iterations. After iterations are over, print the\n",
    "# final cluster means for each of the clusters.\n",
    "# COMMAND 1: Import required libraries\n",
    "import seaborn as sns    # If this fails, use: import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# COMMAND 2: Load IRIS dataset\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "print(\"First 5 rows of IRIS dataset:\")\n",
    "print(iris.head())\n",
    "\n",
    "# COMMAND 3: Select numeric columns for clustering\n",
    "data = iris[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]].values\n",
    "print(\"\\nData shape:\", data.shape)\n",
    "\n",
    "# COMMAND 4: Set number of clusters K = 3\n",
    "K = 3\n",
    "\n",
    "# COMMAND 5: Randomly choose 3 initial cluster means from data points\n",
    "np.random.seed(4)\n",
    "initial_indices = np.random.choice(len(data), K, replace=False)\n",
    "means = data[initial_indices]\n",
    "\n",
    "print(\"\\nInitial Random Cluster Means:\")\n",
    "print(means)\n",
    "\n",
    "# COMMAND 6: Function to compute Euclidean distance\n",
    "def euclidean(p, q):\n",
    "    return np.sqrt(np.sum((p - q)**2))\n",
    "\n",
    "# COMMAND 7: K-means iteration (run for 10 iterations)\n",
    "for iteration in range(10):\n",
    "    print(f\"\\n--- ITERATION {iteration+1} ---\")\n",
    "    \n",
    "    # Step 1: Assign each point to nearest cluster\n",
    "    clusters = {i: [] for i in range(K)}\n",
    "\n",
    "    for point in data:\n",
    "        # compute distance from point to each mean\n",
    "        distances = [euclidean(point, mean) for mean in means]\n",
    "        cluster_index = np.argmin(distances)\n",
    "        clusters[cluster_index].append(point)\n",
    "\n",
    "    # Step 2: Recalculate new means\n",
    "    new_means = []\n",
    "    for i in range(K):\n",
    "        if len(clusters[i]) > 0:\n",
    "            new_mean = np.mean(clusters[i], axis=0)\n",
    "        else:\n",
    "            new_mean = means[i]   # if empty cluster\n",
    "        new_means.append(new_mean)\n",
    "\n",
    "    # Convert list to numpy array\n",
    "    new_means = np.array(new_means)\n",
    "\n",
    "    print(\"Updated Cluster Means:\")\n",
    "    print(new_means)\n",
    "\n",
    "    # Update means\n",
    "    means = new_means\n",
    "\n",
    "# COMMAND 8: Print final means after 10 iterations\n",
    "print(\"\\n==============================\")\n",
    "print(\"FINAL CLUSTER MEANS (AFTER 10 ITERATIONS--------)\")\n",
    "print(\"==============================\")\n",
    "for i, mean in enumerate(means):\n",
    "    print(f\"Cluster {i+1} mean =\", mean)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 21:Write a program to cluster a set of points using K-means for IRIS\n",
    "# dataset. Consider, K=4, clusters. Consider Euclidean distance as the\n",
    "# distance measure. Randomly initialize a cluster mean as one of the data\n",
    "# points. Iterate at least for 10 iterations. After iterations are over, print the\n",
    "# final cluster means for each of the clusters.\n",
    "# COMMAND 1: Import required libraries\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# COMMAND 2: Load the IRIS dataset\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "print(\"First 5 rows:\")\n",
    "print(iris.head())\n",
    "\n",
    "# COMMAND 3: Extract only numeric columns for K-means\n",
    "data = iris[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]].values\n",
    "print(\"\\nData Shape =\", data.shape)\n",
    "\n",
    "# COMMAND 4: Set number of clusters K = 4\n",
    "K = 4\n",
    "\n",
    "# COMMAND 5: Randomly initialize cluster means from dataset rows\n",
    "np.random.seed(10)  # For reproducibility\n",
    "initial_indices = np.random.choice(len(data), K, replace=False)\n",
    "means = data[initial_indices]\n",
    "\n",
    "print(\"\\nInitial Random Cluster Means:\")\n",
    "print(means)\n",
    "\n",
    "# COMMAND 6: Euclidean Distance Function\n",
    "def euclidean(p, q):\n",
    "    return np.sqrt(np.sum((p - q) ** 2))\n",
    "\n",
    "# COMMAND 7: Run K-Means for 10 iterations\n",
    "for iteration in range(10):\n",
    "    print(f\"\\n--- ITERATION {iteration + 1} ---\")\n",
    "    \n",
    "    # Assign points to nearest mean\n",
    "    clusters = {i: [] for i in range(K)}\n",
    "    for point in data:\n",
    "        distances = [euclidean(point, mean) for mean in means]\n",
    "        cluster_index = np.argmin(distances)\n",
    "        clusters[cluster_index].append(point)\n",
    "\n",
    "    # Compute new means\n",
    "    new_means = []\n",
    "    for i in range(K):\n",
    "        if len(clusters[i]) > 0:\n",
    "            new_means.append(np.mean(clusters[i], axis=0))\n",
    "        else:\n",
    "            new_means.append(means[i])  # keep old mean if cluster empty\n",
    "\n",
    "    new_means = np.array(new_means)\n",
    "    print(\"Updated Means:\")\n",
    "    print(new_means)\n",
    "\n",
    "    # Update means\n",
    "    means = new_means\n",
    "\n",
    "# COMMAND 8: Print final cluster means\n",
    "print(\"\\n==============================\")\n",
    "print(\"FINAL CLUSTER MEANS (AFTER 10 ITERATIONS)\")\n",
    "print(\"==============================\")\n",
    "for i, mean in enumerate(means):\n",
    "    print(f\"Cluster {i+1} Mean =\", mean)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
